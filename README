This is the accompanying code for the article 'A new approach to multilabel
stratified cross validation'. The experiments presented in the article can be
replicated by following the steps below:

1. Install dependencies.

python3 -m venv .env && source .env/bin/activate && pip install -r requirements.txt

2. Run the experiments. (note that 10 experiments are run in parallel)

bash run_experiments.sh

3. Results will be generated to 'results' directory. The cross validation
method comparison results (Table 2 in the article) can be found in the files
'results/mean_scores*.csv' Evaluation metric experiment results (Figure 1) are
in 'results/*.pdf'.

The file cv_balance.py contains an implementation of the Optisplit algorithm.
It can be used in the following manner:

```
from cv_balance import optisplit
from cv_comparison_experiment import load_datasets

y = load_datasets('small')['delicious']
folds = optisplit(n_splits=5, targets=y)
```
